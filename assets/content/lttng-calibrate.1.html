<pre>
       The <b>lttng calibrate </b>commands quantifies the overhead of LTTng
       tracers.

       The <b>lttng calibrate </b>command can be used to find out the combined
       average overhead of the LTTng tracers and the instrumentation
       mechanisms used. This overhead can be calibrated in terms of time
       or using any of the PMU performance counter available on the
       system.

       For now, the only implemented calibration is the Linux kernel
       function instrumentation (<i>kretprobes</i>).

   <b>Calibrate Linux kernel function instrumentation</b>
       As an example, we use an i7 processor with 4 general-purpose PMU
       registers. This information is available by issuing <b>dmesg</b>,
       looking for <b>generic registers</b>.

       The following sequence of commands gathers a trace executing a
       kretprobe hooked on an empty function, gathering PMU counters LLC
       (Last Level Cache) misses information (use <b>lttng add-context</b>
       <b>--list </b>to get the list of available PMU counters).

           lttng create calibrate-function
           lttng enable-event calibrate --kernel \
                                        --function=lttng_calibrate_kretprobe
           lttng add-context --kernel --type=perf:cpu:LLC-load-misses \
                                      --type=perf:cpu:LLC-store-misses \
                                      --type=perf:cpu:LLC-prefetch-misses
           lttng start

           for a in $(seq 1 10); do
               lttng calibrate --kernel --function
           done

           lttng destroy
           babeltrace $(ls -1drt ~/lttng-traces/calibrate-function-* | tail -n 1)

       The output from <b>babeltrace</b>(1) can be saved to a text file and
       opened in a spreadsheet (for example, in LibreOffice) to focus on
       the per-PMU counter delta between consecutive <b>calibrate_entry </b>and
       <b>calibrate_return </b>events. Note that these counters are per-CPU, so
       scheduling events would need to be present to account for
       migration between CPUs. Therefore, for calibration purposes, only
       events staying on the same CPU must be considered.

       Here’s an example of the average result, for the i7, on 10
       samples:

       ┌─────────────────────────┬─────────┬────────────────────┐
       │<b>PMU counter              </b>│ <b>Average </b>│ <b>Standard deviation </b>│
       ├─────────────────────────┼─────────┼────────────────────┤
       │                         │         │                    │
       │<b>perf_LLC_load_misses     </b>│ 5.0     │ 0.577              │
       ├─────────────────────────┼─────────┼────────────────────┤
       │                         │         │                    │
       │<b>perf_LLC_store_misses    </b>│ 1.6     │ 0.516              │
       ├─────────────────────────┼─────────┼────────────────────┤
       │                         │         │                    │
       │<b>perf_LLC_prefetch_misses </b>│ 9.0     │ 14.742             │
       └─────────────────────────┴─────────┴────────────────────┘

       As we can notice, the load and store misses are relatively stable
       across runs (their standard deviation is relatively low) compared
       to the prefetch misses. We could conclude from this information
       that LLC load and store misses can be accounted for quite
       precisely, but prefetches within a function seems to behave too
       erratically (not much causality link between the code executed
       and the CPU prefetch activity) to be accounted for.
</pre>